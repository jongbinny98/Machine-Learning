{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jongbinny98/ucsc/blob/master/resNet152.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NegAkmLZKITL"
      },
      "source": [
        "# CSE - 144 Assignment 4\n",
        "\n",
        "## Due: June 7, 2022 11:59 pm\n",
        "\n",
        "\n",
        "**Be sure to set your Runtime environment to include a GPU, as it will speed up the training considerably (this time that's important!).**\n",
        "\n",
        "Intro Slides: https://docs.google.com/presentation/d/1PjqwL9g8XPr40LLRjRAAtxzc4Tf9iqVVnq61gJ59_Iw/edit?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XMaBDrqJAzi",
        "outputId": "3cd67c44-9c66-44b4-8602-f5b2420f4775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "UQ7vc7_hKITR"
      },
      "outputs": [],
      "source": [
        "# Ignore the warnings - Otherwise, TensorFlow tends to innundate one with far too many warnings.\n",
        "import warnings\n",
        "warnings.filterwarnings('always')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Data visualizaton.\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "import seaborn as sns\n",
        "import random as rn\n",
        "\n",
        "# Configure some defaults.\n",
        "%matplotlib inline  \n",
        "style.use('fivethirtyeight')\n",
        "sns.set(style='whitegrid',color_codes=True)\n",
        "\n",
        "# ML + Deep Learning Imports\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "from tensorflow import keras\n",
        "from keras.regularizers import l2\n",
        "from keras.preprocessing.image import ImageDataGenerator # Data Augmentation\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from tensorflow.keras.layers import UpSampling2D\n",
        "from keras.layers.advanced_activations import PReLU\n",
        "from tensorflow.keras.models import Sequential # This building the models\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Conv2D, MaxPooling2D, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD, Adagrad, Adadelta, RMSprop #Optimizers in machine learning are used to tune the parameters of a neural network in order to minimize the cost function\n",
        "from tensorflow.keras.utils import to_categorical # if label is 0,1,...,99 etc then it becomes [0,...1,.,0] a len 100 vector\n",
        "from keras.callbacks import ReduceLROnPlateau #learning rate decay policy\n",
        "from sklearn.model_selection import train_test_split # for splitting data\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd # for making our csv\n",
        "import time\n",
        "# Image preprocessing and reading in.\n",
        "import imageio \n",
        "from pathlib import Path\n",
        "import os, sys\n",
        "from zipfile import ZipFile\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVLfTcsJKITR"
      },
      "source": [
        "#### Step-0: Import dataset\n",
        "Download Tiny-ImageNet-100 dataset using the code blocks below. \n",
        "\n",
        "Please fill in the code block below to split the data into training, validation and test datasets you may use scikit-learn to split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp7dVNpkfC3_",
        "outputId": "fe2e47cf-6099-4cc3-a627-ab5d923ffa7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1bn9RtCsMu-v_ZagKCK2z7hVEDOIjb9Pd\n",
            "To: /content/tiny-image-net-100.zip\n",
            "100% 139M/139M [00:00<00:00, 221MB/s]\n",
            "not found\n",
            "not found\n",
            "not found\n",
            "not found\n",
            "not found\n"
          ]
        }
      ],
      "source": [
        "# Download Tiny-Imagenet-100\n",
        "!gdown 1bn9RtCsMu-v_ZagKCK2z7hVEDOIjb9Pd\n",
        "\n",
        "for file in os.listdir(os.getcwd()):\n",
        "    if file.endswith(\".zip\"):\n",
        "        zip = ZipFile(file)\n",
        "        zip.extractall()\n",
        "    else:\n",
        "        print(\"not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "MItoUh2LZSsX"
      },
      "outputs": [],
      "source": [
        "path = 'tiny-image-net-100/'\n",
        "\n",
        "def get_id_dictionary():\n",
        "    \"\"\"\n",
        "    Maps each class id to an unique integer.\n",
        "    \"\"\"\n",
        "    id_dict = {}\n",
        "    for i, line in enumerate(open(path + 'wnids.txt', 'r')):\n",
        "        id_dict[line.rstrip('\\n')] = i\n",
        "    return id_dict\n",
        "  \n",
        "def get_class_to_id_dict():\n",
        "    \"\"\"\n",
        "    Maps each class id to the English version of the label.\n",
        "    \"\"\"\n",
        "    id_dict = get_id_dictionary()\n",
        "    all_classes = {}\n",
        "    result = {}\n",
        "    for i, line in enumerate(open( path + 'words.txt', 'r')):\n",
        "        n_id, word = line.split('\\t')[:2]\n",
        "        all_classes[n_id] = word\n",
        "    for key, value in id_dict.items():\n",
        "        result[value] = (key, all_classes[key])      \n",
        "    return result\n",
        "\n",
        "def get_data(id_dict, n_samples = 500):\n",
        "    \"\"\"\n",
        "    n_samples: number of samples per class. n_samples has a max of 500.\n",
        "    The range is [1, 500].\n",
        "    \"\"\"\n",
        "    print('starting loading data')\n",
        "    train_data, test_data = [], []\n",
        "    train_labels = []\n",
        "    t = time.time()\n",
        "    for key, value in id_dict.items():\n",
        "      if value<100: # Only focus on first 100 classes\n",
        "        train_data += [imageio.imread( path + 'train/{}/images/{}_{}.JPEG'.format(key, key, str(i)), pilmode='RGB') for i in range(n_samples)]\n",
        "        train_labels_ = np.array([[0]*100]*n_samples)\n",
        "        train_labels_[:, value] = 1\n",
        "        train_labels += train_labels_.tolist()\n",
        "\n",
        "    test_image_names = []\n",
        "    path_list = list(Path(path+'test/images/').glob('*.jpg'))\n",
        "    for test_image_path in path_list:\n",
        "        img_name = str(test_image_path).split('.')[0][-18:]\n",
        "        test_image_names.append(img_name)\n",
        "        test_data.append(imageio.imread(test_image_path , pilmode='RGB'))\n",
        "        \n",
        "    print('finished loading data, in {} seconds'.format(time.time() - t))\n",
        "\n",
        "    train_data = np.array(train_data)\n",
        "    train_labels = np.array(train_labels)\n",
        "    test_data = np.array(test_data)\n",
        "\n",
        "    return train_data, train_labels, test_data, test_image_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6PznhssKITR",
        "outputId": "97a5c379-1095-410d-fde3-fe75bd553e55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "starting loading data\n"
          ]
        }
      ],
      "source": [
        "# Start with n_samples = 10 to get your code working and then increase accordlingy.\n",
        "id_dict = get_id_dictionary()\n",
        "train_data, train_labels, test_data, test_image_names = get_data(id_dict)\n",
        "x_test, test_size = test_data, 0.25\n",
        "x_train, x_val, y_train, y_val = train_test_split(train_data, train_labels, test_size = test_size)\n",
        "\n",
        "\n",
        "print( \"train data shape: \",  x_train.shape )\n",
        "print( \"train label shape: \", y_train.shape )\n",
        "print( \"val data shape: \",  x_val.shape )\n",
        "print( \"val label shape: \", y_val.shape )\n",
        "print( \"test data shape: \",   x_test.shape )\n",
        "# print( \" test label shape: \" len(test_image_names))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSrUa9R4iUU6"
      },
      "outputs": [],
      "source": [
        "# Let's set some random seeds to make this more reproducible.\n",
        "def setseeds():\n",
        "  np.random.seed(137)\n",
        "  rn.seed(137)\n",
        "  tf.random.set_seed(137)\n",
        "setseeds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJvMZiSoprph"
      },
      "source": [
        "#### Step-1: Data Preparation & Exploration\n",
        "\n",
        "Let's take a look at a few of these images. Rerun this cell multiple times to see different images for each class.\n",
        "\n",
        "You may notice that these images look low fidelity, which is because they are! As we increase our image size, we also increase our model complexity. What's important is that our classes are still distinguishable from each other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d45hvQNDKITS"
      },
      "outputs": [],
      "source": [
        "# Visulize one image from Tiny-ImageNet\n",
        "plt.imshow(x_train[0], cmap=plt.cm.binary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFpoh2XBKITT"
      },
      "source": [
        "#### Step-2: Build a neural network.\n",
        "Build your convolutional neural networks by adding some layers. You should use 2 convolution layers and ReLU as the default activation function.\n",
        "Add max pooling after the first layer.\n",
        "The kernel size of both layers should be 3x3. Use 32 and 64 as the number of filters for the first and the second convolutional layers, respectively. After that, flatten your input and add two more dense layers. There should be 1024 units in the first dense with ReLU activation, and use 100 hidden units in the second dense layer with softmax activation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qZrWN65ZbF1"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# build convolutional neural network\n",
        "pre_model = keras.applications.ResNet152(weights='imagenet',\n",
        "                                         include_top = False,\n",
        "                                         input_shape = (256, 256, 3)\n",
        "                                         )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XE-O8w1HWmxo"
      },
      "outputs": [],
      "source": [
        "# for layer in model.layers:\n",
        "#     layer.trainable = False\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "\n",
        "# Now add layers to our pre-trained base model and add classification layers on top of it\n",
        "# x = pre_model.output\n",
        "# x = Flatten()(x)\n",
        "# x = Dense(1024, activation = 'relu')(x)\n",
        "# predic = Dense(100, activation = 'softmax')(x) \n",
        "\n",
        "# # And now put this all together to create our new model.\n",
        "# model = Model(inputs = pre_model.input, outputs = predic) \n",
        "# model.summary()\n",
        "\n",
        "#--------------------------------------------------------------------------------------------\n",
        "\n",
        "# reshapeing\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "\n",
        "model.add(UpSampling2D((2, 2)))\n",
        "\n",
        "# pre-built model \n",
        "model.add(pre_model)\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "#relu = rectified linear unit activation function\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "\n",
        "# dropout (might help overfitting)\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(100, activation='softmax'))\n",
        "\n",
        "model.build(input_shape=(None, 64, 64, 3))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KmndNeeHHD5"
      },
      "source": [
        "### Side Note: How to save a model to google drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMzmHqxtHGgX"
      },
      "outputs": [],
      "source": [
        "# Mount colab to your drive\n",
        "# from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD22-ZNKHjdz"
      },
      "outputs": [],
      "source": [
        "# Add load model if exists\n",
        "# model = keras.models.load_model('/content/drive/My Drive/hw4 kaggle/hw4_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnMotv2THOJL"
      },
      "outputs": [],
      "source": [
        "# Save your model to gdrive\n",
        "model.save('/content/drive/My Drive/hw4 kaggle/hw4_model.h5')  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt27nBzJKITU"
      },
      "source": [
        "#### Step-3: Train the model\n",
        "Compile model here and set your initial hyperparameters. Use ADAM as the optimizer. You should choose 'categorical_crossentropy' as your loss function, and the metrics should be 'accuracy'. After that, train your model for 30 epochs. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vow_Wly8N_AD"
      },
      "outputs": [],
      "source": [
        "# added bc it might help with val accuracy\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=0, patience=5, min_lr=0.5e-3)\n",
        "\n",
        "# model.build(input_shape=(None, 64, 64, 3))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pEyagpLSMLtq"
      },
      "outputs": [],
      "source": [
        "# compile the model\n",
        "model.compile(optimizer = Adam(lr = 1e-4),\n",
        "              loss = 'categorical_crossentropy', \n",
        "              metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PldpcYoHsG-"
      },
      "source": [
        "### Data Augmentation\n",
        "\n",
        "There are many augmentations you can use! Read about them in the Keras documentation.\n",
        "\n",
        " **Rescale is very important!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YenY_mK3LOmp"
      },
      "outputs": [],
      "source": [
        "# Set up data generators for training and validation set\n",
        "# add transformations\n",
        "datagen = ImageDataGenerator(\n",
        "          rescale=1/255.,\n",
        "          featurewise_center=False,           # set input mean to 0 over the dataset\n",
        "          samplewise_center=False,            # set each sample mean to 0\n",
        "          featurewise_std_normalization=False,# divide inputs by std of the dataset\n",
        "          samplewise_std_normalization=False, # divide each input by its std\n",
        "          zca_whitening=False,                # apply ZCA whitening\n",
        "          rotation_range=10,                   # randomly rotate images in the range (degrees, 0 to 180)\n",
        "          width_shift_range=0.2,              # randomly shift images horizontally (fraction of total width)\n",
        "          height_shift_range=0.2,             # randomly shift images vertically (fraction of total height)\n",
        "          horizontal_flip=True,               # randomly flip images\n",
        "          vertical_flip=True)               # randomly flip images\n",
        "\n",
        "\n",
        "valid_datagen = ImageDataGenerator(rescale=1/255.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLAj2KeOLuR1"
      },
      "outputs": [],
      "source": [
        "# fit generators to datasets\n",
        "datagen.fit(x_train)\n",
        "datagen.fit(x_val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1I6RSsDML98I"
      },
      "outputs": [],
      "source": [
        "#set number of epochs, set batch_size\n",
        "#if batch size is too large, it might stuck in localmimima also proventing jumping around\n",
        "# batch = 32 ----> 45%, 64 ----> 43% 16 ------> 46%// probably stuck in local minima() i was doing same thing over and over bc it load previous model\n",
        "\n",
        "# result: lowest to highest\n",
        "# image net ----> 46%\n",
        "# resNet 50 ------> 60%\n",
        "# resNet101 -----> 71%\n",
        "# resNet152 -----> 72%\n",
        "\n",
        "nb_epoch = 20\n",
        "\n",
        "# needs to be at most 32 for certain transfering model\n",
        "batch_size= 32\n",
        "\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "setseeds()\n",
        "\n",
        "# train model \n",
        "history = model.fit(datagen.flow(x_train, y_train, batch_size = batch_size),\n",
        "         validation_data = datagen.flow(x_val, y_val, batch_size = batch_size),\n",
        "         callbacks=[lr_reducer],\n",
        "         steps_per_epoch=len(x_train) / batch_size, epochs=nb_epoch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7umFV7g_rUqC"
      },
      "source": [
        "### Plot Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUmhu6zyrXKf"
      },
      "outputs": [],
      "source": [
        "# Plot Model Performance\n",
        "def plot_data(history):\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model Loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend(['train', 'test'])\n",
        "  plt.show()\n",
        "\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model Accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.legend(['train', 'test'])\n",
        "  plt.show()\n",
        "\n",
        "plot_data(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be60AJz3dbm2"
      },
      "outputs": [],
      "source": [
        "# Make Submission Kaggle File\n",
        "def create_submission_file(model):\n",
        "  name2idx = {}\n",
        "  sample_submission = pd.read_csv('tiny-image-net-100/submission_sample.csv')\n",
        "  filename_order = sample_submission['img_id'].values\n",
        "  for i in range(len(filename_order)):\n",
        "    name2idx[filename_order[i]] = i\n",
        "\n",
        "  # Google colab reads the files in a different order than the answer file was created.\n",
        "  # This is done to preserve the file order.\n",
        "  result_dict = {'img_id': [None]*len(x_test),\n",
        "                'label':[None]*len(x_test)}\n",
        "  test_preds = np.argmax(model.predict(x_test/255.),axis=-1)\n",
        "\n",
        "  for i in range(len(test_image_names)):\n",
        "    test_image_name = test_image_names[i]\n",
        "    result_dict['img_id'][name2idx[test_image_name]] = test_image_name\n",
        "    result_dict['label'][name2idx[test_image_name]] = test_preds[i]\n",
        "\n",
        "  pd.DataFrame(result_dict).to_csv('submission.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01m0eGmqdyAb"
      },
      "outputs": [],
      "source": [
        "# Create your submission file and download it.\n",
        "create_submission_file(model)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "resNet152.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "3afe1e4ce7822ba0e325ee279bb4d100dadd903c2abe2139ffec7391692aa1eb"
    },
    "kernelspec": {
      "display_name": "Python 3.6.13 ('zichao')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}